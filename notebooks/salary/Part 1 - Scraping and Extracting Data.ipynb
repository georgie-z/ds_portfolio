{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as ureq\n",
    "from urllib.request import urlopen\n",
    "import urllib.parse, urllib.error\n",
    "import requests\n",
    "from scrapy.selector import Selector\n",
    "from scrapy.http import HtmlResponse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define search parameters\n",
    "\n",
    "searched_words = ['Data-Scientist-jobs','Data-Science-jobs','Data-Analyst-jobs','Data-Engineer-jobs','Data-Analytics-jobs', 'Analytics-jobs','Data-Insights-jobs','Big-Data-jobs','Business-Intelligence-jobs','Data-Warehouse-jobs','Data-Consultant-jobs']\n",
    "seek_base_url = 'https://www.seek.com.au/'\n",
    "pages = list(range(1,10))\n",
    "\n",
    "# Concatenate base_url, searched word values and page numbers to define the url for all pages\n",
    "\n",
    "searched_url_all = [str(seek_base_url) + words + '/in-All-Melbourne-VIC?page=' + str(i) for i in pages for words in searched_words]\n",
    "searched_url_150k = [str(seek_base_url) + words + '/in-All-Melbourne-VIC?page=' + str(i) + '&salaryrange=150000-999999&salarytype=annual' for i in pages for words in searched_words]\n",
    "searched_url_100k = [str(seek_base_url) + words + '/in-All-Melbourne-VIC?page=' + str(i) + '&salaryrange=100000-999999&salarytype=annual' for i in pages for words in searched_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape searched urls and save as BeautifulSoup objects\n",
    "\n",
    "soup_objects = []\n",
    "\n",
    "for urls in searched_url_100k:\n",
    "    soup = BeautifulSoup(ureq.urlopen(urls), \"lxml\")\n",
    "    soup_objects.append(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1286"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract all job ids and append to job_ids\n",
    "\n",
    "job_ids = []\n",
    "\n",
    "for soup in soup_objects:\n",
    "    job_id = Selector(text=soup.prettify()).xpath('//article/@data-job-id').extract()\n",
    "    \n",
    "    for ids in job_id:\n",
    "        job_ids.append(ids)\n",
    "len(job_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1286"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract individual urls for each job_id\n",
    "\n",
    "job_base_url = 'https://www.seek.com.au/job/'\n",
    "\n",
    "# Concatenate base_url and job_id values to define the url for each job ad\n",
    "\n",
    "job_id_url = [job_base_url + x for x in job_ids]\n",
    "\n",
    "len(job_id_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1286"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract soup objects from each job_id\n",
    "\n",
    "job_soup_objects = []\n",
    "\n",
    "for job_urls in job_id_url:\n",
    "    soups = BeautifulSoup(ureq.urlopen(job_urls), \"lxml\")\n",
    "    job_soup_objects.append(soups)\n",
    "    #time.sleep(1)\n",
    "\n",
    "len(job_soup_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1286\n",
      "1286\n",
      "1286\n",
      "1286\n",
      "1286\n",
      "1286\n",
      "1286\n",
      "1286\n",
      "1286\n"
     ]
    }
   ],
   "source": [
    "# Extract required elements from all the individual job_id_url\n",
    "\n",
    "job_advertiser_list = []\n",
    "job_title_list = []\n",
    "job_class_list = []\n",
    "job_sub_class_list = []\n",
    "job_type_list = []\n",
    "job_location_list = []\n",
    "job_post_date_list = []\n",
    "job_salary_list = []\n",
    "job_desc_list = []\n",
    "\n",
    "for job_soup in job_soup_objects:\n",
    "    \n",
    "    ext_job_advertiser = Selector(text=job_soup.prettify()).xpath('//div/h2/span[2]/span/text()').extract()\n",
    "    job_advertiser_list.append(ext_job_advertiser)\n",
    "    \n",
    "    ext_job_title = Selector(text=job_soup.prettify()).xpath('//span/span/h1/text()').extract()\n",
    "    job_title_list.append(ext_job_title)\n",
    "    \n",
    "    ext_job_class = Selector(text=job_soup.prettify()).xpath('//aside/span/div[2]/div/section/dl/div[2]/dd/span/span/strong/text()').extract()\n",
    "    job_class_list.append(ext_job_class)\n",
    "    \n",
    "    ext_job_sub_class = Selector(text=job_soup.prettify()).xpath('//aside/span/div[2]/div/section/dl/div/dd/span/span/span/text()').extract()\n",
    "    job_sub_class_list.append(ext_job_sub_class)\n",
    "    \n",
    "    ext_job_type = Selector(text=job_soup.prettify()).xpath('//aside/span/div[2]/div/section/dl/dd[3]/span/span').extract()\n",
    "    job_type_list.append(ext_job_type)\n",
    "    \n",
    "    ext_job_location = Selector(text=job_soup.prettify()).xpath('//aside/span/div[2]/div/section/dl/dd[2]/span/span/strong').extract()\n",
    "    job_location_list.append(ext_job_location)\n",
    "    \n",
    "    ext_job_post_date = Selector(text=job_soup.prettify()).xpath('//article/div/div[2]/div/section/dl/dd[1]/span/span').extract()\n",
    "    job_post_date_list.append(ext_job_post_date)\n",
    "    \n",
    "    ext_job_salary = Selector(text=job_soup.prettify()).xpath('//aside/span/div[2]/div/section/dl/div[1]/dd/span/span').extract()\n",
    "    job_salary_list.append(ext_job_salary)\n",
    "    \n",
    "    ext_job_desc = Selector(text=job_soup.prettify()).xpath('//article/div/div/div/span/div/div/div/div/div').extract()\n",
    "    job_desc_list.append(ext_job_desc)\n",
    "    \n",
    "print(len(job_advertiser_list))\n",
    "print(len(job_title_list))\n",
    "print(len(job_class_list))\n",
    "print(len(job_sub_class_list))\n",
    "print(len(job_type_list))\n",
    "print(len(job_location_list))\n",
    "print(len(job_post_date_list))\n",
    "print(len(job_salary_list))\n",
    "print(len(job_desc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_ids</th>\n",
       "      <th>job_advertiser</th>\n",
       "      <th>job_title</th>\n",
       "      <th>job_class</th>\n",
       "      <th>job_sub_class</th>\n",
       "      <th>job_type</th>\n",
       "      <th>job_location</th>\n",
       "      <th>job_post_date</th>\n",
       "      <th>job_salary</th>\n",
       "      <th>job_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36617279</td>\n",
       "      <td>[\\n                   SEEK\\n                  ]</td>\n",
       "      <td>[\\n                   Data Scientist\\n        ...</td>\n",
       "      <td>[\\n                     Science &amp; Technology\\n...</td>\n",
       "      <td>[\\n                     , \\n                  ...</td>\n",
       "      <td>[&lt;span class=\"lwHBT6d\"&gt;\\n                   Fu...</td>\n",
       "      <td>[&lt;strong class=\"lwHBT6d\"&gt;\\n                   ...</td>\n",
       "      <td>[&lt;span class=\"lwHBT6d\"&gt;\\n                    3...</td>\n",
       "      <td>[&lt;span class=\"lwHBT6d\"&gt;\\n                    B...</td>\n",
       "      <td>[&lt;div class=\"templatetext\"&gt;\\n                 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36823796</td>\n",
       "      <td>[\\n                   Morgan Consulting\\n     ...</td>\n",
       "      <td>[\\n                   Data Scientist -\\n      ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[\\n                     , \\n                  ...</td>\n",
       "      <td>[&lt;span class=\"lwHBT6d\"&gt;\\n                   Co...</td>\n",
       "      <td>[&lt;strong class=\"lwHBT6d\"&gt;\\n                   ...</td>\n",
       "      <td>[&lt;span class=\"lwHBT6d\"&gt;\\n                    2...</td>\n",
       "      <td>[&lt;span class=\"\"&gt;\\n                    &lt;strong ...</td>\n",
       "      <td>[&lt;div class=\"templatetext\"&gt;\\n                 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    job_ids                                     job_advertiser  \\\n",
       "0  36617279    [\\n                   SEEK\\n                  ]   \n",
       "1  36823796  [\\n                   Morgan Consulting\\n     ...   \n",
       "\n",
       "                                           job_title  \\\n",
       "0  [\\n                   Data Scientist\\n        ...   \n",
       "1  [\\n                   Data Scientist -\\n      ...   \n",
       "\n",
       "                                           job_class  \\\n",
       "0  [\\n                     Science & Technology\\n...   \n",
       "1                                                 []   \n",
       "\n",
       "                                       job_sub_class  \\\n",
       "0  [\\n                     , \\n                  ...   \n",
       "1  [\\n                     , \\n                  ...   \n",
       "\n",
       "                                            job_type  \\\n",
       "0  [<span class=\"lwHBT6d\">\\n                   Fu...   \n",
       "1  [<span class=\"lwHBT6d\">\\n                   Co...   \n",
       "\n",
       "                                        job_location  \\\n",
       "0  [<strong class=\"lwHBT6d\">\\n                   ...   \n",
       "1  [<strong class=\"lwHBT6d\">\\n                   ...   \n",
       "\n",
       "                                       job_post_date  \\\n",
       "0  [<span class=\"lwHBT6d\">\\n                    3...   \n",
       "1  [<span class=\"lwHBT6d\">\\n                    2...   \n",
       "\n",
       "                                          job_salary  \\\n",
       "0  [<span class=\"lwHBT6d\">\\n                    B...   \n",
       "1  [<span class=\"\">\\n                    <strong ...   \n",
       "\n",
       "                                            job_desc  \n",
       "0  [<div class=\"templatetext\">\\n                 ...  \n",
       "1  [<div class=\"templatetext\">\\n                 ...  "
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataFrame for all, med, high searches\n",
    "\n",
    "med_df = pd.DataFrame(job_ids, columns =['job_ids'])\n",
    "med_df['job_advertiser'] = job_advertiser_list\n",
    "med_df['job_title'] = job_title_list\n",
    "med_df['job_class'] = job_class_list\n",
    "med_df['job_sub_class'] = job_sub_class_list\n",
    "med_df['job_type'] = job_type_list\n",
    "med_df['job_location'] = job_location_list\n",
    "med_df['job_post_date'] = job_post_date_list\n",
    "med_df['job_salary'] = job_salary_list\n",
    "med_df['job_desc'] = job_desc_list\n",
    "\n",
    "med_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save high_df to csv\n",
    "\n",
    "high_df.to_csv('C:/Users/Georgie/Desktop/DSI/project-four/high_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all_df to csv\n",
    "all_df.to_csv('C:/Users/Georgie/Desktop/DSI/project-four/all_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save med_df to csv\n",
    "med_df.to_csv('C:/Users/Georgie/Desktop/DSI/project-four/med_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The end - please refer to Part 2 notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
